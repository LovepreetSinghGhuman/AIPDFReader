{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import Necessary Libraries\n",
    "\n",
    "Make sure  Microsoft Visual C++ is installed on your pc\n",
    "\n",
    "Extracting text from pdf and converting to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# PDF Text Extraction and Topic Modeling\\n\\nThis program extracts text from PDFs, cleans it, and performs topic modeling using BERTopic.\\n\\n## Dependencies\\n- pdfplumber\\n- pandas\\n- nltk\\n- spacy\\n- bertopic\\n- langdetect\\n- wordsegment\\n\\n## Usage\\n1. Place your PDFs in the `papers` folder.\\n2. Run the script to extract, clean, and analyze the text.\\n3. The cleaned data will be saved to `csv/all_cleaned_pdf.csv`.\\n4. Topic modeling results will be visualized using BERTopic.\\n\\n\\npip install pdfplumber bertopic nltk pandas spacy numpy matplotlib wordcloud gensim scikit-learn scipy tqdm wordsegment langdetect\\npython -m spacy download fr_core_web_sm\\npython -m spacy download nl_core_web_md\\npython -m spacy download en_core_web_md\\nUse the following Python version 3.12.0 otherwise the code will not work due to spacy.\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# PDF Text Extraction and Topic Modeling\n",
    "\n",
    "This program extracts text from PDFs, cleans it, and performs topic modeling using BERTopic.\n",
    "\n",
    "Process steps\n",
    "1. Extract text from either a single or multiple research study PDFs. Path is 'studies/papers'\n",
    "2. Clean it using spaCy with language support for english, dutch and french.\n",
    "3. The cleaned data will be saved to 'csv/all_cleaned_pdf.csv'.\n",
    "4. Take the data from the the csv and put into a dataframe.\n",
    "5. Remove any sensitive data and patterns\n",
    "6. Initialize and fit BERTopic\n",
    "7. Visualize Topics\n",
    "8. Visualize Topic Hierarchy\n",
    "9. Visualize documents\n",
    "10. Visualize topics full article\n",
    "\n",
    "\n",
    "## Requirements\n",
    "python -m spacy download fr_core_web_sm\n",
    "python -m spacy download nl_core_web_md\n",
    "python -m spacy download en_core_web_md\n",
    "Use the following Python version 3.12.0 otherwise the code will not work due to spacy.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Lovepreet Singh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import logging\n",
    "import os\n",
    "import re\n",
    "from collections import Counter\n",
    "from multiprocessing import Lock\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "import bertopic  # For topic modeling\n",
    "import pandas as pd # For data manipulation\n",
    "import fitz # For PDF text extraction\n",
    "import spacy  # For Dutch and French tokenization\n",
    "from langdetect import detect, LangDetectException  # For language detection\n",
    "from tqdm import tqdm  # For progress tracking\n",
    "from wordsegment import load, segment  # For word segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-24 01:58:47,364 - ERROR - The folder \\papers does not exist.\n",
      "2025-02-24 01:58:47,364 - ERROR - CSV file csv\\all_cleaned_pdf.csv does not exist.\n"
     ]
    }
   ],
   "source": [
    "# Load English word segmentation model\n",
    "load()\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Paths\n",
    "PDF_DIRECTORY = \"papers\"  # Replace with the path to your PDF folder\n",
    "OUTPUT_CSV = \"csv/cleanedpdfsV5.csv\"  # Replace with the desired output CSV file name\n",
    "\n",
    "# Validate directory existence\n",
    "if not os.path.exists(PDF_DIRECTORY):\n",
    "    logging.error(f\"PDF directory does not exist: {PDF_DIRECTORY}\")\n",
    "    exit(1)\n",
    "\n",
    "# Load spaCy models for efficiency\n",
    "NLP_MODELS = {\n",
    "    'en': spacy.load(\"en_core_web_lg\"),\n",
    "    'nl': spacy.load(\"nl_core_news_lg\"),\n",
    "    'fr': spacy.load(\"fr_core_news_lg\")\n",
    "}\n",
    "\n",
    "def validate_pdf_path(pdf_path: str) -> bool:\n",
    "    \"\"\"Validate that the PDF path exists and is readable.\"\"\"\n",
    "    if not os.path.exists(pdf_path) or not os.access(pdf_path, os.R_OK):\n",
    "        logging.error(f\"PDF file is not accessible: {pdf_path}\")\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def extract_text_from_pdf(pdf_path: str) -> str:\n",
    "    \"\"\"Extract text from a PDF file using PyMuPDF.\"\"\"\n",
    "    try:\n",
    "        with fitz.open(pdf_path) as doc:\n",
    "            return \" \".join(page.get_text() for page in doc).strip()\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error extracting text from {pdf_path}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def detect_language(text: str) -> str:\n",
    "    \"\"\"Detect the language of the text using langdetect.\"\"\"\n",
    "    try:\n",
    "        language = detect(text)\n",
    "        return language if language in NLP_MODELS else 'en'\n",
    "    except LangDetectException:\n",
    "        logging.error(\"Language detection failed. Defaulting to 'en'.\")\n",
    "        return 'en'\n",
    "\n",
    "def clean_text(text: str, language: str) -> str:\n",
    "    \"\"\"Clean and tokenize text using spaCy.\"\"\"\n",
    "    nlp = NLP_MODELS.get(language, NLP_MODELS['en'])\n",
    "    doc = nlp(text)\n",
    "    return \" \".join(token.lemma_.lower() for token in doc if not token.is_stop and not token.is_punct)\n",
    "\n",
    "def split_long_words(text: str) -> str:\n",
    "    \"\"\"Split long words into meaningful segments using wordsegment.\"\"\"\n",
    "    return \" \".join(\" \".join(segment(word)) if len(word) > 10 else word for word in text.split())\n",
    "\n",
    "def extract_clean_sentences(pdf_path: str) -> List[Dict[str, str]]:\n",
    "    \"\"\"Extract and clean sentences from a PDF using PyMuPDF and spaCy.\"\"\"\n",
    "    text_data = []\n",
    "    filename = os.path.basename(pdf_path)\n",
    "    headers, footers = Counter(), Counter()\n",
    "    \n",
    "    with fitz.open(pdf_path) as doc:\n",
    "        for page in doc:\n",
    "            lines = page.get_text().split(\"\\n\")\n",
    "            if len(lines) > 2:\n",
    "                headers[lines[0]] += 1\n",
    "                footers[lines[-1]] += 1\n",
    "\n",
    "        common_header = headers.most_common(1)[0][0] if headers else \"\"\n",
    "        common_footer = footers.most_common(1)[0][0] if footers else \"\"\n",
    "\n",
    "        for page_num, page in enumerate(doc, start=1):\n",
    "            lines = page.get_text().split(\"\\n\")\n",
    "            if len(lines) > 2:\n",
    "                if lines[0] == common_header:\n",
    "                    lines.pop(0)\n",
    "                if lines[-1] == common_footer:\n",
    "                    lines.pop(-1)\n",
    "\n",
    "            cleaned_text = re.sub(r\"\\s{2,}\", \" \", \" \".join(lines))\n",
    "            cleaned_text = re.sub(r\"Page \\d+\", \"\", cleaned_text)\n",
    "            sentences = cleaned_text.strip().split('.')\n",
    "            \n",
    "            for sentence in sentences:\n",
    "                cleaned_sentence = split_long_words(sentence.strip())\n",
    "                if cleaned_sentence:\n",
    "                    text_data.append({\n",
    "                        \"filename\": filename, \n",
    "                        \"Page\": page_num, \n",
    "                        \"sentence\": cleaned_sentence,\n",
    "                        })\n",
    "    \n",
    "    return text_data\n",
    "\n",
    "def process_pdfs(pdf_paths: List[str]) -> List[Dict[str, str]]:\n",
    "    \"\"\"Process multiple PDFs sequentially.\"\"\"\n",
    "    results = []\n",
    "    for pdf_path in tqdm(pdf_paths, desc=\"Processing PDFs\"):\n",
    "        if validate_pdf_path(pdf_path):\n",
    "            results.extend(extract_clean_sentences(pdf_path))\n",
    "    return results\n",
    "\n",
    "def write_to_csv(data: List[Dict[str, str]], output_csv: str) -> None:\n",
    "    \"\"\"Write cleaned text data to a CSV file.\"\"\"\n",
    "    os.makedirs(os.path.dirname(output_csv), exist_ok=True)\n",
    "    with open(output_csv, 'w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=[\"filename\", \"Page\", \"sentence\"])\n",
    "        writer.writeheader()\n",
    "        writer.writerows(data)\n",
    "    logging.info(f\"Saved cleaned data to {output_csv}\")\n",
    "\n",
    "# Get list of PDF paths\n",
    "pdf_paths = [os.path.join(PDF_DIRECTORY, file) for file in os.listdir(PDF_DIRECTORY) if file.endswith(\".pdf\")]\n",
    "\n",
    "# Process PDFs and extract text\n",
    "cleaned_texts = process_pdfs(pdf_paths)\n",
    "\n",
    "# Write cleaned text to CSV\n",
    "if cleaned_texts:\n",
    "    write_to_csv(cleaned_texts, OUTPUT_CSV)\n",
    "    df = pd.read_csv(OUTPUT_CSV)\n",
    "    print(df.head(30))\n",
    "else:\n",
    "    logging.error(\"No cleaned text data to save.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.  Load Your Data\n",
    "\n",
    "Load the articles from your CSV file using pandas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'csv/all_cleaned_pdf.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Load the data\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m df\u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcsv/all_cleaned_pdf.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m df\u001b[38;5;241m.\u001b[39mhead()\n",
      "File \u001b[1;32mc:\\Users\\Lovepreet Singh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Lovepreet Singh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Users\\Lovepreet Singh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Lovepreet Singh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32mc:\\Users\\Lovepreet Singh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'csv/all_cleaned_pdf.csv'"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "df = pd.read_csv(OUTPUT_CSV)\n",
    "df.head(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Prepare Your Text Data\n",
    "We clean up the text\n",
    "- Remove the name of city, country, geography for better outcome\n",
    "- Remove special characters (only letters)\n",
    "- Convert to lower case\n",
    "- Remove stop words\n",
    "- Remove words of only one or 2 letters ('a', 'I', at,...)\n",
    "- Remove very short sentences\n",
    "- Remove urls \n",
    "- use stemming\n",
    "- do duplicate sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precompiled regex patterns for faster matching\n",
    "PATTERNS = {\n",
    "    \"EMAIL\": re.compile(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b'),\n",
    "    \"URL\": re.compile(r'\\bhttps?://\\S+\\b'),\n",
    "    \"PHONE\": re.compile(r'\\b(?:\\+?\\d{1,3}[-.\\s]?)?(?:\\(?\\d{2,3}\\)?[-.\\s]?)?\\d{3,4}[-.\\s]?\\d{4}\\b'),\n",
    "    \"DATE\": [\n",
    "        re.compile(r'\\b\\d{1,4}[-/]\\d{1,2}[-/]\\d{1,4}\\b'),  # yyyy-mm-dd, dd-mm-yyyy, etc.\n",
    "        re.compile(r'\\b\\d{1,2}\\s(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]*\\s\\d{2,4}\\b')  # 1 Jan 2023\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Named Entity categories to redact\n",
    "SENSITIVE_ENTITIES = {\"PER\", \"DATE\", \"GPE\", \"LOC\", \"FAC\", \"ORG\"}\n",
    "\n",
    "def contains_sensitive_info(text, language):\n",
    "    \"\"\"Check if text contains sensitive information.\"\"\"\n",
    "    if not text:\n",
    "        return False\n",
    "    \n",
    "    nlp = nlp_models.get(language, nlp_models['en'])\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # Check for sensitive entities using spaCy's NER\n",
    "    if any(ent.label_ in SENSITIVE_ENTITIES for ent in doc.ents):\n",
    "        return True\n",
    "    \n",
    "    # Check for sensitive patterns using regex\n",
    "    for key, pattern in PATTERNS.items():\n",
    "        if isinstance(pattern, list):\n",
    "            if any(sub_pattern.search(text) for sub_pattern in pattern):\n",
    "                return True\n",
    "        else:\n",
    "            if pattern.search(text):\n",
    "                return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "def remove_sensitive_info(text, language):\n",
    "    \"\"\"Remove sensitive information from the text.\"\"\"\n",
    "    if not text:\n",
    "        return text\n",
    "    \n",
    "    nlp = nlp_models.get(language, nlp_models['en'])\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # Redact sensitive entities\n",
    "    redacted_text = []\n",
    "    for token in doc:\n",
    "        if token.ent_type_ in SENSITIVE_ENTITIES:\n",
    "            redacted_text.append(\"[REDACTED]\")\n",
    "        else:\n",
    "            redacted_text.append(token.text)\n",
    "    \n",
    "    redacted_text = ' '.join(redacted_text)\n",
    "    \n",
    "    # Redact sensitive patterns using regex\n",
    "    for key, pattern in PATTERNS.items():\n",
    "        if isinstance(pattern, list):\n",
    "            for sub_pattern in pattern:\n",
    "                redacted_text = sub_pattern.sub(\"[REDACTED]\", redacted_text)\n",
    "        else:\n",
    "            redacted_text = pattern.sub(\"[REDACTED]\", redacted_text)\n",
    "    \n",
    "    return redacted_text\n",
    "\n",
    "# Example usage with a DataFrame\n",
    "df['Contains_Sensitive_Info'] = df['Cleaned_Text'].apply(lambda x: contains_sensitive_info(x, 'en'))\n",
    "df['Redacted_Text'] = df['Cleaned_Text'].apply(lambda x: remove_sensitive_info(x, 'en'))\n",
    "df.head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load spaCy's English NER model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Function to remove geographic entities (cities, countries, locations)\n",
    "def remove_geographical_entities(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"  # Handle missing or non-string values\n",
    "    \n",
    "    doc = nlp(text)\n",
    "    filtered_tokens = [token.text for token in doc if token.ent_type_ not in [\"GPE\", \"LOC\", \"FAC\"]]\n",
    "    \n",
    "    return \" \".join(filtered_tokens)\n",
    "\n",
    "# Apply function to remove cities, countries, and geography\n",
    "df['sentence_clean'] = df['sentence'].apply(remove_geographical_entities)\n",
    "\n",
    "# Display a few cleaned sentences\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the spaCy model (use 'en_core_web_sm' for small model)\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Minimum word length threshold\n",
    "MIN_WORD_SIZE = 4\n",
    "\n",
    "# Preprocessing function to clean sentences\n",
    "def preprocess_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"  # Handle missing or non-string values\n",
    "    \n",
    "    # Remove URLs (http, https, www, etc.)\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    \n",
    "    # Normalize Unicode characters and remove diacritics\n",
    "    text = unicodedata.normalize('NFKD', text)\n",
    "    text = ''.join([c for c in text if not unicodedata.combining(c)])\n",
    "    \n",
    "    # Replace non-alphabetic characters with a space\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', ' ', text)\n",
    "\n",
    "    # Process the text with spaCy\n",
    "    doc = nlp(text.lower())\n",
    "\n",
    "    # Extract tokens, remove stopwords, and apply lemmatization\n",
    "    tokens = [\n",
    "        token.lemma_ for token in doc\n",
    "        if not token.is_stop  # Remove stopwords\n",
    "        and not token.is_punct  # Remove punctuation\n",
    "        and not token.is_space  # Remove spaces\n",
    "        and len(token.lemma_) >= MIN_WORD_SIZE  # Remove short words\n",
    "    ]\n",
    "\n",
    "    # Remove duplicate words within each sentence (preserve order)\n",
    "    unique_tokens = list(dict.fromkeys(tokens))\n",
    "\n",
    "    # Ensure proper spacing between words\n",
    "    return \" \".join(unique_tokens)\n",
    "\n",
    "# Apply preprocessing function to clean sentences\n",
    "df['sentence_clean'] = df['sentence_clean'].apply(preprocess_text)\n",
    "\n",
    "# Remove duplicate sentences (keep one instance of each)\n",
    "df = df.drop_duplicates(subset=['sentence_clean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first element of the cleaned text\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sentence_clean'].is_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['sentence_clean'].str.len() >= 15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To see how data cleaning looks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the output path\n",
    "output_path = r\"cleaned_data.csv\"\n",
    "\n",
    "# Save the cleaned DataFrame to CSV\n",
    "df.to_csv(output_path, index=False, encoding='utf-8')\n",
    "\n",
    "print(f\" Cleaned DataFrame saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Initialize and Fit BERTopic\n",
    "The good thing with BERTopic is that is does most of the work automatically (Meaning, I do not need to bore you to death with details about how it works behind te scenes.)\n",
    "\n",
    "We need to do 3 things\n",
    "1. Initialize BERTopic model\n",
    "2. 'Fit' the model -> this  means: run the model, as you would run a simple linear regression\n",
    "3. Look at the topics via \n",
    "\n",
    "To get started, let's just use the default settings."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
