{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import Necessary Libraries\n",
    "\n",
    "Make sure  Microsoft Visual C++ is installed on your pc\n",
    "\n",
    "Extracting text from pdf and converting to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nNeeded dependancies:\\n    - spacy\\n    - pdfplumber\\n    - bertopic\\n    - pandas\\n    - numpy\\n    - matplotlib\\n    - wordcloud\\n    - nltk\\n    - gensim\\n    - scikit-learn\\n    - scipy\\n    - tqdm\\n    - umap-learn\\n    - hdbscan\\n    - sentence-transformers\\n    - transformers\\n    - torch\\n    - tensorflow\\n    - tensorflow_hub\\n    - tensorflow_text\\npip install spacy pdfplumber bertopic\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Needed dependancies:\n",
    "    - spacy\n",
    "    - pdfplumber\n",
    "    - bertopic\n",
    "    - pandas\n",
    "    - numpy\n",
    "    - matplotlib\n",
    "    - wordcloud\n",
    "    - nltk\n",
    "    - gensim\n",
    "    - scikit-learn\n",
    "    - scipy\n",
    "    - tqdm\n",
    "    - umap-learn\n",
    "    - hdbscan\n",
    "    - sentence-transformers\n",
    "    - transformers\n",
    "    - torch\n",
    "    - tensorflow\n",
    "    - tensorflow_hub\n",
    "    - tensorflow_text\n",
    "pip install spacy pdfplumber bertopic\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'spacy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtqdm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tqdm  \u001b[38;5;66;03m# For progress tracking\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangdetect\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m detect  \u001b[38;5;66;03m# For language detection\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mspacy\u001b[39;00m  \u001b[38;5;66;03m# For Dutch and French tokenization\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mbertopic\u001b[39;00m \u001b[38;5;66;03m# For topic modeling\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'spacy'"
     ]
    }
   ],
   "source": [
    "import pdfplumber\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from wordsegment import load, segment\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "from tqdm import tqdm  # For progress tracking\n",
    "from langdetect import detect  # For language detection\n",
    "import spacy  # For Dutch and French tokenization\n",
    "import bertopic # For topic modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure nltk sentence tokenizer is downloaded\n",
    "nltk.download('punkt', quiet=True)\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# Load English word segmentation model\n",
    "load()\n",
    "\n",
    "# Load spaCy models for Dutch and French\n",
    "nlp_nl = spacy.load(\"nl_core_news_sm\")  # Dutch\n",
    "nlp_fr = spacy.load(\"fr_core_news_sm\")  # French\n",
    "\n",
    "# Folder containing the PDFs\n",
    "pdf_folder = r\"Studies\"\n",
    "output_csv_path = r\"csv/all_cleaned_pdf.csv\"\n",
    "\n",
    "# Compile regex patterns for efficiency\n",
    "EXTRA_SPACES_PATTERN = re.compile(r\"\\s{2,}\")\n",
    "PAGE_NUM_PATTERN = re.compile(r\"Page \\d+\")\n",
    "LINE_BREAKS_PATTERN = re.compile(r\"\\n+\")\n",
    "\n",
    "# Function to detect language of a text\n",
    "def detect_language(text):\n",
    "    try:\n",
    "        return detect(text)\n",
    "    except:\n",
    "        return \"en\"  # Default to English if detection fails\n",
    "\n",
    "# Function to split long merged words into meaningful words (English only)\n",
    "def split_long_words(text, language=\"en\"):\n",
    "    if not isinstance(text, str) or language != \"en\":\n",
    "        return text  # Return as is if not a string or not English\n",
    "    \n",
    "    words = text.split()  # Split text into individual words\n",
    "    processed_text = []\n",
    "\n",
    "    for word in words:\n",
    "        if len(word) > 10:  # Threshold for long words\n",
    "            segmented_word = \" \".join(segment(word))  # Use wordsegment to break into words\n",
    "            processed_text.append(segmented_word)\n",
    "        else:\n",
    "            processed_text.append(word)\n",
    "    \n",
    "    return \" \".join(processed_text)\n",
    "\n",
    "# Function to tokenize sentences based on language\n",
    "def tokenize_sentences(text, language):\n",
    "    if language == \"en\":\n",
    "        return sent_tokenize(text)  # Use nltk for English\n",
    "    elif language == \"nl\":\n",
    "        doc = nlp_nl(text)  # Use spaCy for Dutch\n",
    "        return [sent.text for sent in doc.sents]\n",
    "    elif language == \"fr\":\n",
    "        doc = nlp_fr(text)  # Use spaCy for French\n",
    "        return [sent.text for sent in doc.sents]\n",
    "    else:\n",
    "        return sent_tokenize(text)  # Default to nltk for other languages\n",
    "\n",
    "# Function to extract and clean sentences from a PDF\n",
    "def extract_clean_sentences_from_pdf(pdf_path):\n",
    "    text_data = []\n",
    "    headers = Counter()\n",
    "    footers = Counter()\n",
    "    filename = os.path.basename(pdf_path)  # Extract file name\n",
    "\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        # Detect common headers/footers\n",
    "        for page in pdf.pages:\n",
    "            text = page.extract_text()\n",
    "            if text:\n",
    "                lines = text.split(\"\\n\")\n",
    "                if len(lines) > 2:\n",
    "                    headers[lines[0]] += 1  # First line as potential header\n",
    "                    footers[lines[-1]] += 1  # Last line as potential footer\n",
    "\n",
    "        # Identify the most common header/footer\n",
    "        common_header = headers.most_common(1)[0][0] if headers else \"\"\n",
    "        common_footer = footers.most_common(1)[0][0] if footers else \"\"\n",
    "\n",
    "        # Extract and clean text\n",
    "        for page_num, page in enumerate(pdf.pages):\n",
    "            text = page.extract_text()\n",
    "\n",
    "            if text:\n",
    "                lines = text.split(\"\\n\")\n",
    "\n",
    "                # Remove detected headers and footers\n",
    "                if len(lines) > 2:\n",
    "                    if lines[0] == common_header:\n",
    "                        lines.pop(0)  # Remove header\n",
    "                    if lines[-1] == common_footer:\n",
    "                        lines.pop(-1)  # Remove footer\n",
    "\n",
    "                # Join cleaned lines back into text\n",
    "                cleaned_text = \"\\n\".join(lines)\n",
    "\n",
    "                # Further cleanup: remove excessive spaces, page numbers, and metadata\n",
    "                cleaned_text = EXTRA_SPACES_PATTERN.sub(\" \", cleaned_text)  # Remove extra spaces\n",
    "                cleaned_text = PAGE_NUM_PATTERN.sub(\"\", cleaned_text)  # Remove page numbers\n",
    "                cleaned_text = LINE_BREAKS_PATTERN.sub(\" \", cleaned_text)  # Remove extra line breaks\n",
    "\n",
    "                # Detect language of the text\n",
    "                language = detect_language(cleaned_text)\n",
    "\n",
    "                # Tokenize into sentences using language-specific tokenizer\n",
    "                sentences = tokenize_sentences(cleaned_text.strip(), language)\n",
    "\n",
    "                # Save each sentence as a separate row with filename\n",
    "                for sentence in sentences:\n",
    "                    cleaned_sentence = split_long_words(sentence, language)  # Apply word segmentation (English only)\n",
    "                    text_data.append({\n",
    "                        \"filename\": filename,\n",
    "                        \"Page\": page_num + 1,\n",
    "                        \"language\": language,\n",
    "                        \"sentence\": cleaned_sentence\n",
    "                    })\n",
    "\n",
    "    return text_data\n",
    "\n",
    "# Function to process a single PDF and return its data\n",
    "def process_pdf(pdf_file):\n",
    "    pdf_path = os.path.join(pdf_folder, pdf_file)\n",
    "    try:\n",
    "        return extract_clean_sentences_from_pdf(pdf_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {pdf_file}: {e}\")\n",
    "        return []\n",
    "\n",
    "# Function to process all PDFs in the folder and save to CSV\n",
    "def process_all_pdfs(pdf_folder, output_csv_path):\n",
    "    all_text_data = []\n",
    "\n",
    "    # Get list of PDF files\n",
    "    pdf_files = [f for f in os.listdir(pdf_folder) if f.endswith(\".pdf\")]\n",
    "\n",
    "    # Use multiprocessing to process PDFs in parallel\n",
    "    with ProcessPoolExecutor() as executor:\n",
    "        futures = {executor.submit(process_pdf, pdf_file): pdf_file for pdf_file in pdf_files}\n",
    "        for future in tqdm(as_completed(futures), total=len(pdf_files), desc=\"Processing PDFs\"):\n",
    "            all_text_data.extend(future.result())\n",
    "\n",
    "    # Save to a single CSV file\n",
    "    df = pd.DataFrame(all_text_data)\n",
    "\n",
    "    # Ensure output directory exists\n",
    "    os.makedirs(os.path.dirname(output_csv_path), exist_ok=True)\n",
    "\n",
    "    df.to_csv(output_csv_path, index=False, encoding='utf-8')\n",
    "    print(f\"All PDFs processed! CSV saved at: {output_csv_path}\")\n",
    "\n",
    "# Run the function to process all PDFs\n",
    "process_all_pdfs(pdf_folder, output_csv_path)\n",
    "\n",
    "# Load CSV and display first few rows\n",
    "df = pd.read_csv(output_csv_path)\n",
    "\n",
    "# Display the cleaned DataFrame\n",
    "print(\" Extracted and Cleaned Sentences DataFrame:\")\n",
    "print(df.head())  # Display the first few rows of cleaned sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.  Load Your Data\n",
    "\n",
    "Load the articles from your CSV file using pandas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "df= pd.read_csv(r'csv/all_cleaned_pdf.csv')\n",
    "df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
