{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import Necessary Libraries\n",
    "\n",
    "Make sure  Microsoft Visual C++ is installed on your pc\n",
    "\n",
    "Extracting text from pdf and converting to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nNeeded dependancies:\\n    - spacy\\n    - pdfplumber\\n    - bertopic\\n    - pandas\\n    - numpy\\n    - matplotlib\\n    - wordcloud\\n    - nltk\\n    - gensim\\n    - scikit-learn\\n    - scipy\\n    - tqdm\\npip install pdfplumber bertopic nltk pandas spacy\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Needed dependancies:\n",
    "    - spacy\n",
    "    - pdfplumber\n",
    "    - bertopic\n",
    "    - pandas\n",
    "    - numpy\n",
    "    - matplotlib\n",
    "    - wordcloud\n",
    "    - nltk\n",
    "    - gensim\n",
    "    - scikit-learn\n",
    "    - scipy\n",
    "    - tqdm\n",
    "pip install pdfplumber bertopic nltk pandas spacy\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpdfplumber\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mre\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import pdfplumber\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from wordsegment import load, segment\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "from tqdm import tqdm  # For progress tracking\n",
    "from langdetect import detect  # For language detection\n",
    "import spacy  # For Dutch and French tokenization\n",
    "import bertopic # For topic modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure nltk sentence tokenizer is downloaded\n",
    "nltk.download('punkt', quiet=True)\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# Load English word segmentation model\n",
    "load()\n",
    "\n",
    "# Load spaCy models for Dutch and French\n",
    "nlp_nl = spacy.load(\"nl_core_news_sm\")  # Dutch\n",
    "nlp_fr = spacy.load(\"fr_core_news_sm\")  # French\n",
    "\n",
    "# Folder containing the PDFs\n",
    "pdf_folder = r\"papers\"\n",
    "output_csv_path = r\"csv/all_cleaned_pdf.csv\"\n",
    "\n",
    "# Compile regex patterns for efficiency\n",
    "EXTRA_SPACES_PATTERN = re.compile(r\"\\s{2,}\")\n",
    "PAGE_NUM_PATTERN = re.compile(r\"Page \\d+\")\n",
    "LINE_BREAKS_PATTERN = re.compile(r\"\\n+\")\n",
    "\n",
    "# Function to detect language of a text\n",
    "def detect_language(text):\n",
    "    try:\n",
    "        return detect(text)\n",
    "    except:\n",
    "        return \"en\"  # Default to English if detection fails\n",
    "\n",
    "# Function to split long merged words into meaningful words (English only)\n",
    "def split_long_words(text, language=\"en\"):\n",
    "    if not isinstance(text, str) or language != \"en\":\n",
    "        return text  # Return as is if not a string or not English\n",
    "    \n",
    "    words = text.split()  # Split text into individual words\n",
    "    processed_text = []\n",
    "\n",
    "    for word in words:\n",
    "        if len(word) > 10:  # Threshold for long words\n",
    "            segmented_word = \" \".join(segment(word))  # Use wordsegment to break into words\n",
    "            processed_text.append(segmented_word)\n",
    "        else:\n",
    "            processed_text.append(word)\n",
    "    \n",
    "    return \" \".join(processed_text)\n",
    "\n",
    "# Function to tokenize sentences based on language\n",
    "def tokenize_sentences(text, language):\n",
    "    if language == \"en\":\n",
    "        return sent_tokenize(text)  # Use nltk for English\n",
    "    elif language == \"nl\":\n",
    "        doc = nlp_nl(text)  # Use spaCy for Dutch\n",
    "        return [sent.text for sent in doc.sents]\n",
    "    elif language == \"fr\":\n",
    "        doc = nlp_fr(text)  # Use spaCy for French\n",
    "        return [sent.text for sent in doc.sents]\n",
    "    else:\n",
    "        return sent_tokenize(text)  # Default to nltk for other languages\n",
    "\n",
    "# Function to extract and clean sentences from a PDF\n",
    "def extract_clean_sentences_from_pdf(pdf_path):\n",
    "    text_data = []\n",
    "    headers = Counter()\n",
    "    footers = Counter()\n",
    "    filename = os.path.basename(pdf_path)  # Extract file name\n",
    "\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        # Detect common headers/footers\n",
    "        for page in pdf.pages:\n",
    "            text = page.extract_text()\n",
    "            if text:\n",
    "                lines = text.split(\"\\n\")\n",
    "                if len(lines) > 2:\n",
    "                    headers[lines[0]] += 1  # First line as potential header\n",
    "                    footers[lines[-1]] += 1  # Last line as potential footer\n",
    "\n",
    "        # Identify the most common header/footer\n",
    "        common_header = headers.most_common(1)[0][0] if headers else \"\"\n",
    "        common_footer = footers.most_common(1)[0][0] if footers else \"\"\n",
    "\n",
    "        # Extract and clean text\n",
    "        for page_num, page in enumerate(pdf.pages):\n",
    "            text = page.extract_text()\n",
    "\n",
    "            if text:\n",
    "                lines = text.split(\"\\n\")\n",
    "\n",
    "                # Remove detected headers and footers\n",
    "                if len(lines) > 2:\n",
    "                    if lines[0] == common_header:\n",
    "                        lines.pop(0)  # Remove header\n",
    "                    if lines[-1] == common_footer:\n",
    "                        lines.pop(-1)  # Remove footer\n",
    "\n",
    "                # Join cleaned lines back into text\n",
    "                cleaned_text = \"\\n\".join(lines)\n",
    "\n",
    "                # Further cleanup: remove excessive spaces, page numbers, and metadata\n",
    "                cleaned_text = EXTRA_SPACES_PATTERN.sub(\" \", cleaned_text)  # Remove extra spaces\n",
    "                cleaned_text = PAGE_NUM_PATTERN.sub(\"\", cleaned_text)  # Remove page numbers\n",
    "                cleaned_text = LINE_BREAKS_PATTERN.sub(\" \", cleaned_text)  # Remove extra line breaks\n",
    "\n",
    "                # Detect language of the text\n",
    "                language = detect_language(cleaned_text)\n",
    "\n",
    "                # Tokenize into sentences using language-specific tokenizer\n",
    "                sentences = tokenize_sentences(cleaned_text.strip(), language)\n",
    "\n",
    "                # Save each sentence as a separate row with filename\n",
    "                for sentence in sentences:\n",
    "                    cleaned_sentence = split_long_words(sentence, language)  # Apply word segmentation (English only)\n",
    "                    text_data.append({\n",
    "                        \"filename\": filename,\n",
    "                        \"Page\": page_num + 1,\n",
    "                        \"language\": language,\n",
    "                        \"sentence\": cleaned_sentence\n",
    "                    })\n",
    "\n",
    "    return text_data\n",
    "\n",
    "# Function to process a single PDF and return its data\n",
    "def process_pdf(pdf_file):\n",
    "    pdf_path = os.path.join(pdf_folder, pdf_file)\n",
    "    try:\n",
    "        return extract_clean_sentences_from_pdf(pdf_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {pdf_file}: {e}\")\n",
    "        return []\n",
    "\n",
    "# Function to process all PDFs in the folder and save to CSV\n",
    "def process_all_pdfs(pdf_folder, output_csv_path):\n",
    "    all_text_data = []\n",
    "\n",
    "    # Get list of PDF files\n",
    "    pdf_files = [f for f in os.listdir(pdf_folder) if f.endswith(\".pdf\")]\n",
    "\n",
    "    # Use multiprocessing to process PDFs in parallel\n",
    "    with ProcessPoolExecutor() as executor:\n",
    "        futures = {executor.submit(process_pdf, pdf_file): pdf_file for pdf_file in pdf_files}\n",
    "        for future in tqdm(as_completed(futures), total=len(pdf_files), desc=\"Processing PDFs\"):\n",
    "            all_text_data.extend(future.result())\n",
    "\n",
    "    # Save to a single CSV file\n",
    "    df = pd.DataFrame(all_text_data)\n",
    "\n",
    "    # Ensure output directory exists\n",
    "    os.makedirs(os.path.dirname(output_csv_path), exist_ok=True)\n",
    "\n",
    "    df.to_csv(output_csv_path, index=False, encoding='utf-8')\n",
    "    print(f\"All PDFs processed! CSV saved at: {output_csv_path}\")\n",
    "\n",
    "# Run the function to process all PDFs\n",
    "process_all_pdfs(pdf_folder, output_csv_path)\n",
    "\n",
    "# Load CSV and display first few rows\n",
    "df = pd.read_csv(output_csv_path)\n",
    "\n",
    "# Display the cleaned DataFrame\n",
    "print(\" Extracted and Cleaned Sentences DataFrame:\")\n",
    "print(df.head())  # Display the first few rows of cleaned sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.  Load Your Data\n",
    "\n",
    "Load the articles from your CSV file using pandas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "df= pd.read_csv(r'csv/all_cleaned_pdf.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing any personal information to anonymize data  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load spaCy model for Named Entity Recognition (NER)\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Function to remove personal names, dates, and numbers\n",
    "def remove_sensitive_info(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"  # Handle non-string values\n",
    "\n",
    "    doc = nlp(text)\n",
    "\n",
    "    # Remove PERSON names using spaCy NER\n",
    "    words = [token.text for token in doc if token.ent_type_ != \"PERSON\"]\n",
    "\n",
    "    # Remove dates and numbers\n",
    "    cleaned_text = \" \".join(words)\n",
    "    cleaned_text = re.sub(r'\\b\\d{1,4}[-/]\\d{1,2}[-/]\\d{1,4}\\b', '', cleaned_text)  # Remove dates (YYYY-MM-DD, DD/MM/YYYY)\n",
    "    cleaned_text = re.sub(r'\\b\\d+\\b', '', cleaned_text)  # Remove standalone numbers\n",
    "\n",
    "    return cleaned_text.strip()\n",
    "\n",
    "# Apply function to remove names, personal info, dates, and numbers from df['sentence_clean']\n",
    "df['sentence_clean'] = df['sentence'].apply(remove_sensitive_info)\n",
    "\n",
    "# Display cleaned dataframe\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Prepare Your Text Data\n",
    "We clean up the text\n",
    "- Remove the name of city, country, geography for better outcome\n",
    "- Remove special characters (only letters)\n",
    "- Convert to lower case\n",
    "- Remove stop words\n",
    "- Remove words of only one or 2 letters ('a', 'I', at,...)\n",
    "- Remove very short sentences\n",
    "- Remove urls \n",
    "- use stemming\n",
    "- do duplicate sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load spaCy's English NER model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Function to remove geographic entities (cities, countries, locations)\n",
    "def remove_geographical_entities(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"  # Handle missing or non-string values\n",
    "    \n",
    "    doc = nlp(text)\n",
    "    filtered_tokens = [token.text for token in doc if token.ent_type_ not in [\"GPE\", \"LOC\", \"FAC\"]]\n",
    "    \n",
    "    return \" \".join(filtered_tokens)\n",
    "\n",
    "# Apply function to remove cities, countries, and geography\n",
    "df['sentence_clean'] = df['sentence'].apply(remove_geographical_entities)\n",
    "\n",
    "# Display a few cleaned sentences\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure necessary NLTK resources are downloaded\n",
    "download('wordnet')\n",
    "download('stopwords')\n",
    "\n",
    "# Load stopwords (fully remove stopwords first)\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Minimum word length threshold\n",
    "minWordSize = 4\n",
    "\n",
    "# Initialize the WordNetLemmatizer and PorterStemmer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Preprocessing function to clean sentences\n",
    "def preprocess_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"  # Handle missing or non-string values\n",
    "    \n",
    "    # Remove URLs (http, https, www, etc.)\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    \n",
    "    # Normalize Unicode characters\n",
    "    text = unicodedata.normalize('NFKD', text)\n",
    "    \n",
    "    # Replace non-alphabetic characters with a space\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', ' ', text)\n",
    "\n",
    "    # Convert text to lowercase and split into words\n",
    "    words = text.lower().split()\n",
    "\n",
    "    # Remove stopwords first\n",
    "    filtered_words = [word for word in words if word not in stop_words]\n",
    "\n",
    "    # Apply lemmatization\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in filtered_words]\n",
    "\n",
    "    # Apply stemming after lemmatization\n",
    "    stemmed_words = [stemmer.stem(word) for word in lemmatized_words]\n",
    "\n",
    "    # Remove short words after stopword removal\n",
    "    final_words = [w for w in stemmed_words if len(w) >= minWordSize]\n",
    "\n",
    "    # Remove duplicate words within each sentence\n",
    "    unique_words = list(dict.fromkeys(final_words))  # Preserves order but removes duplicates\n",
    "\n",
    "    # Ensure proper spacing between words\n",
    "    return \" \".join(unique_words)\n",
    "\n",
    "# Apply preprocessing function to clean sentences\n",
    "df['sentence_clean'] = df['sentence_clean'].apply(preprocess_text)\n",
    "\n",
    "# Identify and remove duplicate sentences\n",
    "sentence_counts = Counter(df['sentence_clean'])\n",
    "\n",
    "# Remove sentences that appear multiple times\n",
    "df = df[df['sentence_clean'].map(sentence_counts) == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first element of the cleaned text\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sentence_clean'].is_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['sentence_clean'].str.len() >= 15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To see how data cleaning looks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the output path\n",
    "output_path = r\"cleaned_data.csv\"\n",
    "\n",
    "# Save the cleaned DataFrame to CSV\n",
    "df.to_csv(output_path, index=False, encoding='utf-8')\n",
    "\n",
    "print(f\" Cleaned DataFrame saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Initialize and Fit BERTopic\n",
    "The good thing with BERTopic is that is does most of the work automatically (Meaning, I do not need to bore you to death with details about how it works behind te scenes.)\n",
    "\n",
    "We need to do 3 things\n",
    "1. Initialize BERTopic model\n",
    "2. 'Fit' the model -> this  means: run the model, as you would run a simple linear regression\n",
    "3. Look at the topics via \n",
    "\n",
    "To get started, let's just use the default settings."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
