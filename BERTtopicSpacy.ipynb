{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import Necessary Libraries\n",
    "\n",
    "Make sure  Microsoft Visual C++ is installed on your pc\n",
    "\n",
    "Extracting text from pdf and converting to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# PDF Text Extraction and Topic Modeling\n",
    "\n",
    "This program extracts text from PDFs, cleans it, and performs topic modeling using BERTopic.\n",
    "\n",
    "Process steps\n",
    "1. Import necessary libraries\n",
    "2. Extract text from either a single or multiple research study PDFs. Path is 'studies/papers'\n",
    "3. Clean it using spaCy with language support for english, dutch and french.\n",
    "4. Perform topic modeling using BERTopic\n",
    "5. Visualize the topics\n",
    "\n",
    "\n",
    "\n",
    "## Requirements\n",
    "python -m spacy download fr_core_web_sm\n",
    "python -m spacy download nl_core_web_md\n",
    "python -m spacy download en_core_web_md\n",
    "Use the following Python version 3.12.0 otherwise the code will not work due to spacy compatibility.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import logging\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "from collections import Counter\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "import bertopic  # For topic modeling\n",
    "import pandas as pd # For data manipulation\n",
    "import fitz # For PDF text extraction\n",
    "import spacy  # For Dutch and French tokenization\n",
    "from langdetect import detect, LangDetectException  # For language detection\n",
    "from tqdm import tqdm  # For progress tracking\n",
    "from wordsegment import load, segment  # For word segmentation\n",
    "from spacy.lang.en.stop_words import STOP_WORDS as EN_STOP_WORDS\n",
    "from spacy.lang.nl.stop_words import STOP_WORDS as NL_STOP_WORDS\n",
    "from spacy.lang.fr.stop_words import STOP_WORDS as FR_STOP_WORDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Extract text from either a single or multiple research study PDFs. Path is 'studies/papers'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration from a JSON file\n",
    "with open(\"config.json\") as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "PDF_DIRECTORY = config[\"pdf_directory\"]  # Folder containing PDFs\n",
    "CLEANED_CSV = config[\"cleaned_csv\"]  # Cleaned CSV path\n",
    "GDPR_CSV = config[\"gdpr_csv\"]  # GDPR CSV path\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Load spaCy models once to avoid repeated initialization\n",
    "NLP_MODELS = {\n",
    "    'en': spacy.load(\"en_core_web_lg\"),\n",
    "    'nl': spacy.load(\"nl_core_news_lg\"),\n",
    "    'fr': spacy.load(\"fr_core_news_lg\"),\n",
    "}\n",
    "\n",
    "# Ligature replacement dictionary\n",
    "LIGATURES = {\n",
    "    'ﬁ': 'fi', 'ﬂ': 'fl', 'ﬃ': 'ffi', 'ﬄ': 'ffl', 'ﬀ': 'ff'\n",
    "}\n",
    "\n",
    "# Common word split fixes\n",
    "COMMON_FIXES = {\n",
    "    'signi ficant': 'significant', 'di fferent': 'different',\n",
    "    'e ffective': 'effective', 'e ffect': 'effect',\n",
    "    'chil dren': 'children', 'e ff ective': 'effective',\n",
    "    'con fi dence': 'confidence'\n",
    "}\n",
    "\n",
    "# Unwanted keywords for filtering\n",
    "UNWANTED_KEYWORDS = {\n",
    "    'doi', 'https', 'http', 'journal', 'university', 'copyrighted',\n",
    "    'taylor & francis', 'elsevier', 'published by', 'received',\n",
    "    'revised', 'author(s)', 'source:', 'history:', 'keywords',\n",
    "    'volume', 'downloaded', 'article', 'creative commons use',\n",
    "    'authors', 'all rights reserved'\n",
    "}\n",
    "\n",
    "# Reference and Acknowledgement markers\n",
    "REFERENCE_MARKERS = {'references', 'bibliography', 'acknowledgements', 'method', 'methods'}\n",
    "\n",
    "def validate_pdf_path(pdf_path: str) -> bool:\n",
    "    \"\"\"Checks if the PDF file exists and is readable.\"\"\"\n",
    "    if not os.path.exists(pdf_path) or not os.access(pdf_path, os.R_OK):\n",
    "        logging.error(f\"PDF file is not accessible: {pdf_path}\")\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def is_heading(line: str) -> bool:\n",
    "    \"\"\"Determines if a line is a heading (all uppercase or starts with 'CHAPTER').\"\"\"\n",
    "    return line.isupper() or line.startswith('CHAPTER')\n",
    "\n",
    "def is_footnote(line: str) -> bool:\n",
    "    \"\"\"Identifies footnotes based on common patterns.\"\"\"\n",
    "    return bool(re.match(r'^\\[\\d+\\]', line) or re.match(r'^\\d+\\.', line) or \n",
    "                line.startswith(('*', 'Note', 'Table')))\n",
    "\n",
    "def contains_doi_or_https(line: str) -> bool:\n",
    "    \"\"\"Checks if a line contains unwanted keywords, DOIs, or URLs.\"\"\"\n",
    "    return any(keyword in line.lower() for keyword in UNWANTED_KEYWORDS)\n",
    "\n",
    "def is_reference_or_acknowledgements_section(line: str) -> bool:\n",
    "    \"\"\"Checks if a line marks the start of references or acknowledgements.\"\"\"\n",
    "    return any(marker in line.lower() for marker in REFERENCE_MARKERS)\n",
    "\n",
    "def replace_ligatures(text: str) -> str:\n",
    "    \"\"\"Replaces ligatures in text with their normal character equivalents.\"\"\"\n",
    "    for lig, replacement in LIGATURES.items():\n",
    "        text = text.replace(lig, replacement)\n",
    "    return text\n",
    "\n",
    "def fix_common_word_splits(text: str) -> str:\n",
    "    \"\"\"Fixes common word splits in text.\"\"\"\n",
    "    for split_word, correct_word in COMMON_FIXES.items():\n",
    "        text = text.replace(split_word, correct_word)\n",
    "    return text\n",
    "\n",
    "def detect_language(text: str) -> str:\n",
    "    \"\"\"Detects the language of a given text block.\"\"\"\n",
    "    try:\n",
    "        return detect(text)\n",
    "    except LangDetectException:\n",
    "        return \"unknown\"\n",
    "\n",
    "def process_pdf(file_path: str, filename: str) -> list:\n",
    "    \"\"\"Processes a single PDF file and extracts structured text data.\"\"\"\n",
    "    data = []\n",
    "    title = os.path.splitext(filename)[0]  # Extract filename without extension\n",
    "    \n",
    "    try:\n",
    "        with fitz.open(file_path) as pdf_document:\n",
    "            section_reached = False  # Stop processing once references start\n",
    "\n",
    "            for page_num in range(pdf_document.page_count):\n",
    "                if section_reached:\n",
    "                    break  # Stop processing once references detected\n",
    "\n",
    "                page = pdf_document.load_page(page_num)\n",
    "                text_dict = page.get_text(\"dict\")  # Extract text while keeping layout\n",
    "\n",
    "                for block in text_dict[\"blocks\"]:\n",
    "                    if block[\"type\"] != 0:  # Ignore non-text blocks\n",
    "                        continue\n",
    "\n",
    "                    paragraph = []\n",
    "                    prev_x = None  # Track x-coordinate for paragraph grouping\n",
    "\n",
    "                    for line in block[\"lines\"]:\n",
    "                        line_text = \" \".join(span[\"text\"].replace(';', ',') for span in line[\"spans\"])\n",
    "                        line_text = replace_ligatures(line_text)\n",
    "                        line_text = fix_common_word_splits(line_text)\n",
    "\n",
    "                        if is_reference_or_acknowledgements_section(line_text):\n",
    "                            section_reached = True\n",
    "                            break  # Stop further processing\n",
    "\n",
    "                        if (is_heading(line_text) or is_footnote(line_text) or \n",
    "                            contains_doi_or_https(line_text) or \n",
    "                            line_text.strip().lower() == title.lower()):\n",
    "                            continue\n",
    "\n",
    "                        first_word_x = line[\"spans\"][0][\"bbox\"][0]\n",
    "\n",
    "                        if prev_x is None or abs(first_word_x - prev_x) < 10:\n",
    "                            paragraph.append(line_text)\n",
    "                        else:\n",
    "                            if paragraph:  # Store completed paragraph\n",
    "                                full_text = \" \".join(paragraph).strip()\n",
    "                                if len(full_text.split()) >= 10:\n",
    "                                    language = detect_language(full_text)\n",
    "                                    data.append([filename, page_num + 1, full_text, language])\n",
    "                            paragraph = [line_text]  # Start new paragraph\n",
    "\n",
    "                        prev_x = first_word_x  # Update for next iteration\n",
    "\n",
    "                    if paragraph and not section_reached:\n",
    "                        full_text = \" \".join(paragraph).strip()\n",
    "                        if len(full_text.split()) >= 10:\n",
    "                            language = detect_language(full_text)\n",
    "                            data.append([filename, page_num + 1, full_text, language])\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to process {file_path}: {e}\")\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "all_data = []\n",
    "\n",
    "pdf_files = [f for f in os.listdir(PDF_DIRECTORY) if f.endswith('.pdf')]\n",
    "\n",
    "for filename in tqdm(pdf_files, desc=\"Processing PDFs\"):\n",
    "    file_path = os.path.join(PDF_DIRECTORY, filename)\n",
    "    if validate_pdf_path(file_path):\n",
    "        all_data.extend(process_pdf(file_path, filename))\n",
    "\n",
    "if all_data:\n",
    "    df_extracted = pd.DataFrame(all_data, columns=[\"File\", \"Page\", \"Text\", \"Language\"])\n",
    "    df_extracted.to_csv(CLEANED_CSV, index=False) \n",
    "    logging.info(f\"Data successfully exported to {CLEANED_CSV}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "df_cleaned = pd.read_csv(CLEANED_CSV)\n",
    "df_cleaned.head(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Clean it using spaCy with language support for english, dutch and french.\n",
    "We clean up the text\n",
    "- Remove the name of city, country, geography for better outcome\n",
    "- Remove special characters (only letters)\n",
    "- Convert to lower case\n",
    "- Remove stop words\n",
    "- Remove words of only one or 2 letters ('a', 'I', at,...)\n",
    "- Remove very short sentences\n",
    "- Remove urls \n",
    "- use stemming\n",
    "- remove duplicate sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define entity types to remove (Personal Information)\n",
    "PERSONAL_ENTITIES = {\"PERSON\", \"EMAIL\", \"PHONE\", \"GPE\", \"ORG\"}\n",
    "\n",
    "def remove_personal_info(text: str, lang: str) -> str:\n",
    "    \"\"\"Uses spaCy NER to remove personal information based on language.\"\"\"\n",
    "    if lang not in NLP_MODELS:\n",
    "        return text  # Skip processing if language model is not available\n",
    "\n",
    "    nlp = NLP_MODELS[lang]  # Select the correct model\n",
    "    doc = nlp(text)\n",
    "\n",
    "    # Replace personal entities with [REDACTED]\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ in PERSONAL_ENTITIES:\n",
    "            text = text.replace(ent.text, \"[REDACTED]\")\n",
    "\n",
    "    # Remove emails and phone numbers using regex (fallback)\n",
    "    text = re.sub(r\"\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b\", \"[REDACTED]\", text)  # Email\n",
    "    text = re.sub(r\"\\b\\d{3}[-.\\s]?\\d{3}[-.\\s]?\\d{4}\\b\", \"[REDACTED]\", text)  # Phone Numbers\n",
    "\n",
    "    return text\n",
    "\n",
    "def anonymize_csv(input_csv: str, output_csv: str):\n",
    "    \"\"\"Reads a CSV file, anonymizes text based on language, and write to a new CSV.\"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(input_csv)\n",
    "\n",
    "        # Apply anonymization function to text column with respective language\n",
    "        df[\"Text\"] = df.apply(lambda row: remove_personal_info(str(row[\"Text\"]), row[\"Language\"]), axis=1)\n",
    "\n",
    "        # Write anonymized data to csv\n",
    "        df.to_csv(output_csv, index=False)\n",
    "        logging.info(f\"Anonymized data saved to {output_csv}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing file: {e}\")\n",
    "\n",
    "# Run anonymization\n",
    "anonymize_csv(CLEANED_CSV, GDPR_CSV)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using SpaCy NER to make the Model GDPR Compliant\n",
    "- Remove any sensitive data and patterns, The GDPR data will be saved in a new csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map languages to their respective spaCy models and stop words\n",
    "LANGUAGE_MODELS = {\n",
    "    'en': (NLP_MODELS['en'], EN_STOP_WORDS),\n",
    "    'nl': (NLP_MODELS['nl'], NL_STOP_WORDS),\n",
    "    'fr': (NLP_MODELS['fr'], FR_STOP_WORDS)\n",
    "}\n",
    "\n",
    "# Function to clean text using spaCy\n",
    "def clean_text(text, language):\n",
    "    if language not in LANGUAGE_MODELS:\n",
    "        return text  # Return the original text if the language is not supported\n",
    "\n",
    "    nlp, stop_words = LANGUAGE_MODELS[language]\n",
    "    doc = nlp(text)\n",
    "\n",
    "    # Lemmatize, remove stop words, and filter out short tokens\n",
    "    cleaned_tokens = [\n",
    "        token.lemma_.lower() for token in doc\n",
    "        if not token.is_stop  # Remove stop words\n",
    "        and not token.is_punct  # Remove punctuation\n",
    "        and not token.is_space  # Remove spaces\n",
    "        and len(token.text) > 3  # Remove short tokens\n",
    "    ]\n",
    "\n",
    "    return \" \".join(cleaned_tokens)\n",
    "\n",
    "# Load the CSV file \n",
    "df_gdpr = pd.read_csv(CLEANED_CSV)\n",
    "\n",
    "# Clean the text in the DataFrame\n",
    "df_gdpr[\"Cleaned_Text\"] = df_gdpr.apply(lambda row: clean_text(row[\"Text\"], row[\"Language\"]), axis=1)\n",
    "\n",
    "# Drop the original \"Text\" column\n",
    "df_gdpr.drop(columns=[\"Text\"], inplace=True)\n",
    "\n",
    "# Save the cleaned DataFrame to a new CSV file\n",
    "df_gdpr.to_csv(CLEANED_CSV, index=False)\n",
    "print(f\"Cleaned data saved to {CLEANED_CSV}\")\n",
    "df_gdpr.head(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Initialize and Fit BERTopic\n",
    "The good thing with BERTopic is that is does most of the work automatically (Meaning, I do not need to bore you to death with details about how it works behind te scenes.)\n",
    "\n",
    "We need to do 3 things\n",
    "1. Initialize BERTopic model\n",
    "2. 'Fit' the model -> this  means: run the model, as you would run a simple linear regression\n",
    "3. Look at the topics via \n",
    "\n",
    "To get started, let's just use the default settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Visualize Topics\n",
    "- Visualize Topic Hierarchy\n",
    "- Visualize documents\n",
    "- Visualize topics full article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
